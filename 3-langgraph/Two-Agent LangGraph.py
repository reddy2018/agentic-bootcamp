from typing import TypedDict, Optional, List, Annotated
from langgraph.graph import START, END, StateGraph # build the directed workflow graph.
from langchain_openai import ChatOpenAI # the LLM client.
from dotenv import load_dotenv
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode, tools_condition 
from langgraph.graph.message import add_messages
from typing import Annotated
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, BaseMessage
from wikipedia import summary as wiki_summary
from langchain_core.prompts import PromptTemplate # builds prompts for the LLM.
from langgraph.checkpoint.memory import MemorySaver 



load_dotenv() #loads environment variables from a .env file (useful for API keys).

# ---------- Tool to fetch Wikipedia content ----------
def wikipedia_tool(topic: str) -> str:
    """
    A simple Wikipedia search tool. input should be a search query.
    """
    try:
        return wiki_summary(topic, sentences=8, auto_suggest=False, redirect=True)
    except Exception as e:
        return f"Could not fetch from wikipedia for {topic}: {str(e)}"

# ---------- Define the workflow state ----------
class workflow_state(TypedDict, total=False):
    query: str
    context: Annotated[str, 'text fetched from wikipedia']
    summary: Annotated[str, 'summary generated by LLM']
    formatted_summary: Annotated[str, 'summary formatted into points']
    source: Annotated[str, 'metadata - where the context was retrieved from']
    
# ---------- Initialize LLM ---------- 
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)

# ---------- Summarizer ----------
def summarize_tool(text: str, source: str = "wikipedia") -> str:
    if not text or 'Could not fetch' in text:
        return "No valid context to summarize."
    prompt = PromptTemplate.from_template("you are a helpful assistant that summarizes technical/contexual text. Summarize the following text in 1-2 lines:\n\n{text}\n\nSummary:")
    chain = prompt | llm # creates a pipeline/chain where the template is passed into the LLM.
    response = chain.invoke({"text": text})
    # add source attribute here
    return f"{response.content.strip()}\n\nSource: {source}"
    #return response.content


# ---------- Research Agent ----------
def research_agent(state: workflow_state) -> workflow_state:
    query = state.get("query", "") # ensure query is a string
    if not query:
        state["summary"] = "No query provided."
        return state
    
    # Step 1: Use wikipedia tool to fetch context
    context = wikipedia_tool(state["query"])
    state["context"] = context
    # set metadata
    state["source"] = "wikipedia"
    return state

# ---------- Summary Agent ----------
def summary_agent(state: workflow_state) -> workflow_state:
    context = state.get("context", "") # ensure context is a string
    if not context:
        state["summary"] = "No context available to summarize."
        return state
    
    # step 2: generate summary
    source = state.get("source", "unknown source")
    summary = summarize_tool(context, source)
    state["summary"] = summary
    return state

# ---------- Formatter Agent ----------
def formatter_agent(state: workflow_state) -> workflow_state:
    """formats the summary text into point"""
    summary = state.get("summary", "")
    if not summary or summary.startswith("No"):
        state["formatted_summary"]
        return state
    
    prompt = PromptTemplate.from_template(
        "format the following summary into clear, concise bullet points"
        "each point should represent one key idea. \n\nSummary:\n{text}\n\nBullet points:"
        )
    chain = prompt | llm
    response = chain.invoke({"text": summary})
    state["formatted_summary"] = response.content 
    return state

# ---------- Build Workflow ----------
def builder_workflow():
    builder = StateGraph(workflow_state)
    builder.add_node("research_agent", research_agent)
    builder.add_node("summary_agent", summary_agent)
    builder.add_node("formatter_agent", formatter_agent)
    builder.add_edge(START, "research_agent")
    builder.add_edge("research_agent", "summary_agent")
    builder.add_edge("summary_agent", "formatter_agent")
    builder.add_edge("formatter_agent", END)
    checkpoint = MemorySaver()
    return builder.compile()


# ---------- Run ----------
if __name__ == "__main__":
    graph = builder_workflow()
    print("wikipedia summarizer agent")
    print("type 'exit' anytime to quit. \n")
    
    while True:
        user_query = input("Enter a topic to summarize: ").strip()
        if user_query.lower() in {"exit", "quit"}:
            print("\n Exiting. have a great day!")
            break
        
        if not user_query:
            print("please enter a valid topic.\n")
            continue
        
        print("\n Fetching and summarizing...Please wait. \n")
        
        try:
            final_state = graph.invoke({"query": user_query})
            print("\n query:", user_query)
            print("\n context from wikipedia: \n", final_state.get('context'))
            print("\n summary from LLM:\n", final_state.get('summary'))
            print("\n formatted summary:\n", final_state.get('formatted_summary'))
        except Exception as e:
            print(f"Error: {e}\n")
    